{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 11 DL Project - Part A "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gilad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-eea201bbdae3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__version__'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'1.15'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TensorBoard logging requires TensorBoard version 1.15 or above'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "import sklearn as sk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "en_stops = set(stopwords.words('english'))\n",
    "####\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will make a new dataset from the files with positive and negative reviews\n",
    "\n",
    "def make_clean_comments(dir_path, comment_type):\n",
    "    #get all the files from the directory\n",
    "    onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n",
    "    files_paths = [dir_path + \"\\\\\" + f for f in onlyfiles]\n",
    "    \n",
    "    comments_tokens = [] #{[tokens_from_1_file]:0/1,[tokens_from_2_file]...} e.t.c\n",
    "    regex = re.compile('[^a-zA-Z]') #regex for removing all the symbols that are not letters\n",
    "    \n",
    "    #for each comment(=file in pos/neg directory)\n",
    "    for path_to_file in files_paths:\n",
    "        with open(path_to_file, encoding=\"utf8\") as f:\n",
    "            full_comment = f.read() #the data from file with comments\n",
    "            words = full_comment.split() \n",
    "            clean_list = [] #this list will contain only words that doesn't have symbols other than letters\n",
    "            \n",
    "            #for each words in comment (=file)\n",
    "            for w in words:\n",
    "                clean_word_data = regex.sub('', w) #clean the word by regex\n",
    "                \n",
    "                if len(clean_word_data) > 1:\n",
    "                    #cleaning the stopwords from the sentence\n",
    "                    clean_word_data = clean_word_data.lower()\n",
    "                    #checking that the word is not a stop word (=a word that shows up frequently)\n",
    "                    if clean_word_data not in en_stops:\n",
    "                        clean_list.append(clean_word_data) #creates a clean comment\n",
    "                        \n",
    "        comments_tokens.append(clean_list)\n",
    "        #creating list of comment type for the later creation of dataset\n",
    "    if comment_type == \"pos\":          \n",
    "        comments_type = [1 for i in range(len(comments_tokens))]\n",
    "    else:\n",
    "        comments_type = [0 for i in range(len(comments_tokens))]\n",
    "\n",
    "    return comments_tokens, comments_type\n",
    "\n",
    "## Positive comments tokenization ##\n",
    "pos_dir_path = fr\"D:\\School\\YearD\\SemesterB\\DeepLearning2\\Project\\train\\pos\"\n",
    "pos_tokens_sentences, comments_type_list_pos = make_clean_comments(pos_dir_path, \"pos\")\n",
    "## Negative comments tokenization ##\n",
    "neg_dir_path = fr\"D:\\School\\YearD\\SemesterB\\DeepLearning2\\Project\\train\\neg\"\n",
    "neg_tokens_sentences, comments_type_list_neg = make_clean_comments(neg_dir_path, \"neg\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the labled data\n",
    "data_pos = {'comment':pos_tokens_sentences,\n",
    "        'lable':comments_type_list_pos}\n",
    "data_neg = {'comment':neg_tokens_sentences,\n",
    "        'lable':comments_type_list_neg}\n",
    "\n",
    "#preparing data for embedding\n",
    "data_for_embedding_w2v = []\n",
    "data_for_embedding_ft = []\n",
    "max_sentence = 0\n",
    "for s in pos_tokens_sentences:\n",
    "    if len(s) > max_sentence:\n",
    "        max_sentence = len(s)\n",
    "    data_for_embedding_w2v.append(s)\n",
    "    data_for_embedding_ft.append(s)\n",
    "for s in neg_tokens_sentences:\n",
    "    if len(s) > max_sentence:\n",
    "        max_sentence = len(s)\n",
    "    data_for_embedding_w2v.append(s)\n",
    "    data_for_embedding_ft.append(s)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a dataset of all the data together\n",
    "df_temp_1 = pd.DataFrame(data_pos, index=range(0,18750))\n",
    "df_temp_2 = pd.DataFrame(data_neg, index=range(18750,37500))\n",
    "dataset = pd.concat([df_temp_1,df_temp_2])\n",
    "\n",
    "\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bromwell, high, cartoon, comedy, ran, time, p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[homelessness, houselessness, george, carlin, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[brilliant, overacting, lesley, ann, warren, b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[easily, underrated, film, inn, brooks, cannon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[typical, mel, brooks, film, much, less, slaps...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[isnt, comedic, robin, williams, quirkyinsane,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[yes, art, successfully, make, slow, paced, th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[critically, acclaimed, psychological, thrille...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[night, listener, robin, williams, toni, colle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[know, robin, williams, god, bless, constantly...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[first, read, armistead, maupins, story, taken...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[liked, film, action, scenes, interesting, ten...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[many, illnesses, born, mind, man, given, life...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[enjoyed, night, listener, much, one, better, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[night, listener, probably, one, williams, bes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[like, one, previous, commenters, said, founda...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[night, listener, held, attention, robin, will...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[popular, radio, storyteller, gabriel, onerobi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[one, thing, recommend, film, intriguing, prem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[absolutely, loved, film, relate, comments, re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[night, listener, better, people, generally, s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[comes, movies, pretty, picky, ill, complain, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[somewhat, funny, wellpaced, action, thriller,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[legendary, boris, karloff, ended, illustrious...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[tale, based, two, edgar, allen, poe, pieces, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[aro, tolbukhin, burnt, alive, seven, people, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[seeing, several, movies, villaronga, pretty, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[theres, many, things, fall, aro, tolbukhin, e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[got, unique, twists, two, genres, ever, seen,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[without, kirsten, miller, project, neednt, co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37470</th>\n",
       "      <td>[remember, watching, movie, several, times, yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37471</th>\n",
       "      <td>[first, scene, problem, child, baby, peeing, n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37472</th>\n",
       "      <td>[kid, rather, bad, way, make, type, outsmarts,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37473</th>\n",
       "      <td>[girlfriend, stunned, bad, film, minutes, woul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37474</th>\n",
       "      <td>[one, worst, movies, ever, seen, saw, toronto,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37475</th>\n",
       "      <td>[recently, reviewed, lipstick, first, time, de...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37476</th>\n",
       "      <td>[dont, waste, minutes, time, fast, food, fast,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37477</th>\n",
       "      <td>[movie, billed, comedy, mystery, fails, badly,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37478</th>\n",
       "      <td>[story, starts, slow, nothing, funny, happens,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37479</th>\n",
       "      <td>[film, massive, yawn, proving, americans, have...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37480</th>\n",
       "      <td>[sunday, night, waiting, advertised, movie, tv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37481</th>\n",
       "      <td>[ever, seen, film, shockingly, inept, think, p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37482</th>\n",
       "      <td>[next, time, party, someone, asks, day, heard,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37483</th>\n",
       "      <td>[turgid, dialogue, feeble, characterization, h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37484</th>\n",
       "      <td>[cameron, diaz, woman, married, judge, played,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37485</th>\n",
       "      <td>[misfortune, watch, rubbish, sky, cinema, max,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37486</th>\n",
       "      <td>[pretty, bad, generic, movie, synopsis, inform...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37487</th>\n",
       "      <td>[watched, movie, scifi, channel, conclude, fil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37488</th>\n",
       "      <td>[first, im, dog, movie, find, totally, enjoyab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37489</th>\n",
       "      <td>[ah, yez, sci, fi, channel, produces, yeti, an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37490</th>\n",
       "      <td>[yeti, curse, snow, demon, starts, aboard, pla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37491</th>\n",
       "      <td>[hmmm, sports, team, plane, crash, gets, stran...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37492</th>\n",
       "      <td>[saw, piece, garbage, amc, last, night, wonder...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37493</th>\n",
       "      <td>[although, production, jerry, jamesons, direct...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37494</th>\n",
       "      <td>[capt, gallagher, lemmon, flight, attendant, e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37495</th>\n",
       "      <td>[towards, end, movie, felt, technical, felt, l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37496</th>\n",
       "      <td>[kind, movie, enemies, content, watch, time, b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37497</th>\n",
       "      <td>[saw, descent, last, night, stockholm, film, f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37498</th>\n",
       "      <td>[films, pick, pound, turn, rather, good, rd, c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37499</th>\n",
       "      <td>[one, dumbest, films, ive, ever, seen, rips, n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  lable\n",
       "0      [bromwell, high, cartoon, comedy, ran, time, p...      1\n",
       "1      [homelessness, houselessness, george, carlin, ...      1\n",
       "2      [brilliant, overacting, lesley, ann, warren, b...      1\n",
       "3      [easily, underrated, film, inn, brooks, cannon...      1\n",
       "4      [typical, mel, brooks, film, much, less, slaps...      1\n",
       "5      [isnt, comedic, robin, williams, quirkyinsane,...      1\n",
       "6      [yes, art, successfully, make, slow, paced, th...      1\n",
       "7      [critically, acclaimed, psychological, thrille...      1\n",
       "8      [night, listener, robin, williams, toni, colle...      1\n",
       "9      [know, robin, williams, god, bless, constantly...      1\n",
       "10     [first, read, armistead, maupins, story, taken...      1\n",
       "11     [liked, film, action, scenes, interesting, ten...      1\n",
       "12     [many, illnesses, born, mind, man, given, life...      1\n",
       "13     [enjoyed, night, listener, much, one, better, ...      1\n",
       "14     [night, listener, probably, one, williams, bes...      1\n",
       "15     [like, one, previous, commenters, said, founda...      1\n",
       "16     [night, listener, held, attention, robin, will...      1\n",
       "17     [popular, radio, storyteller, gabriel, onerobi...      1\n",
       "18     [one, thing, recommend, film, intriguing, prem...      1\n",
       "19     [absolutely, loved, film, relate, comments, re...      1\n",
       "20     [night, listener, better, people, generally, s...      1\n",
       "21     [comes, movies, pretty, picky, ill, complain, ...      1\n",
       "22     [somewhat, funny, wellpaced, action, thriller,...      1\n",
       "23     [legendary, boris, karloff, ended, illustrious...      1\n",
       "24     [tale, based, two, edgar, allen, poe, pieces, ...      1\n",
       "25     [aro, tolbukhin, burnt, alive, seven, people, ...      1\n",
       "26     [seeing, several, movies, villaronga, pretty, ...      1\n",
       "27     [theres, many, things, fall, aro, tolbukhin, e...      1\n",
       "28     [got, unique, twists, two, genres, ever, seen,...      1\n",
       "29     [without, kirsten, miller, project, neednt, co...      1\n",
       "...                                                  ...    ...\n",
       "37470  [remember, watching, movie, several, times, yo...      0\n",
       "37471  [first, scene, problem, child, baby, peeing, n...      0\n",
       "37472  [kid, rather, bad, way, make, type, outsmarts,...      0\n",
       "37473  [girlfriend, stunned, bad, film, minutes, woul...      0\n",
       "37474  [one, worst, movies, ever, seen, saw, toronto,...      0\n",
       "37475  [recently, reviewed, lipstick, first, time, de...      0\n",
       "37476  [dont, waste, minutes, time, fast, food, fast,...      0\n",
       "37477  [movie, billed, comedy, mystery, fails, badly,...      0\n",
       "37478  [story, starts, slow, nothing, funny, happens,...      0\n",
       "37479  [film, massive, yawn, proving, americans, have...      0\n",
       "37480  [sunday, night, waiting, advertised, movie, tv...      0\n",
       "37481  [ever, seen, film, shockingly, inept, think, p...      0\n",
       "37482  [next, time, party, someone, asks, day, heard,...      0\n",
       "37483  [turgid, dialogue, feeble, characterization, h...      0\n",
       "37484  [cameron, diaz, woman, married, judge, played,...      0\n",
       "37485  [misfortune, watch, rubbish, sky, cinema, max,...      0\n",
       "37486  [pretty, bad, generic, movie, synopsis, inform...      0\n",
       "37487  [watched, movie, scifi, channel, conclude, fil...      0\n",
       "37488  [first, im, dog, movie, find, totally, enjoyab...      0\n",
       "37489  [ah, yez, sci, fi, channel, produces, yeti, an...      0\n",
       "37490  [yeti, curse, snow, demon, starts, aboard, pla...      0\n",
       "37491  [hmmm, sports, team, plane, crash, gets, stran...      0\n",
       "37492  [saw, piece, garbage, amc, last, night, wonder...      0\n",
       "37493  [although, production, jerry, jamesons, direct...      0\n",
       "37494  [capt, gallagher, lemmon, flight, attendant, e...      0\n",
       "37495  [towards, end, movie, felt, technical, felt, l...      0\n",
       "37496  [kind, movie, enemies, content, watch, time, b...      0\n",
       "37497  [saw, descent, last, night, stockholm, film, f...      0\n",
       "37498  [films, pick, pound, turn, rather, good, rd, c...      0\n",
       "37499  [one, dumbest, films, ive, ever, seen, rips, n...      0\n",
       "\n",
       "[37500 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the embedding models\n",
    "from gensim.models import FastText\n",
    "\n",
    "model_w2v = gensim.models.word2vec.Word2Vec(sentences=data_for_embedding_w2v, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model_w2v.train(data_for_embedding_w2v, total_examples=model_w2v.corpus_count, epochs=30)\n",
    "if torch.cuda.is_available():\n",
    "    model_w2v.cuda()\n",
    "    \n",
    "model_ft = FastText(sentences=data_for_embedding_ft, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model_ft.train(data_for_embedding_ft, total_examples=model_ft.corpus_count, epochs=30)\n",
    "if torch.cuda.is_available():\n",
    "    model_ft.cuda()\n",
    "    \n",
    "\n",
    "\n",
    "#w2v_weights = torch.FloatTensor(model_w2v.wv.vectors)\n",
    "# Implementing the models into a list\n",
    "#models_lst=[model_w2v,model_ft]\n",
    "\n",
    "#embed_model = models_lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nw2v_X_pp = data_for_embedding.apply(lambda x: make_indexed_torch(x, \"w2v\"))\\nw2v_X = w2v_X_pp.values\\n\\nw2v_y=dataset[\\'lable\\'].to_list\\n\\nft_X_pp = data_for_embedding.apply(lambda x: make_indexed_torch(x, \"ft\"))\\nft_X = ft_X_pp.values\\n\\nft_y=dataset[\\'lable\\'].to_list\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Padding the Embeddings '''\n",
    "\n",
    "def make_indexed_torch(comment, embed_model):\n",
    "    if embed_model == \"w2v\":\n",
    "        \n",
    "        return torch.LongTensor([model_w2v.wv.get_index(word) for word in comment])\n",
    "    else:\n",
    "        return torch.LongTensor([model_ft.wv.get_index(word) for word in comment])\n",
    "\n",
    "def padding_tensors_to_the_same_dim(com_torch, target_size):\n",
    "    nump_torch = com_torch.numpy()\n",
    "    pad_size = target_size - nump_torch.size\n",
    "    nump_torch = np.pad(nump_torch, (0,pad_size), 'constant')\n",
    "    return torch.from_numpy(nump_torch)\n",
    "    \n",
    "    \n",
    "    \n",
    "comments = dataset['comment']\n",
    "w2v_X_pp = comments.apply(lambda x: make_indexed_torch(x, \"w2v\"))\n",
    "w2v_X = w2v_X_pp.apply(lambda x: padding_tensors_to_the_same_dim(x, max_sentence))\n",
    "w2v_y = dataset['lable']\n",
    "\n",
    "\n",
    "#smth = make_indexed_torch(data_for_embedding_w2v[1],\"w2v\")\n",
    "#smth2 = make_indexed_torch(data_for_embedding_ft[1],\"ft\")\n",
    "#print(smth)\n",
    "#print(smth2)\n",
    "\"\"\"\n",
    "w2v_X_pp = data_for_embedding.apply(lambda x: make_indexed_torch(x, \"w2v\"))\n",
    "w2v_X = w2v_X_pp.values\n",
    "\n",
    "w2v_y=dataset['lable'].to_list\n",
    "\n",
    "ft_X_pp = data_for_embedding.apply(lambda x: make_indexed_torch(x, \"ft\"))\n",
    "ft_X = ft_X_pp.values\n",
    "\n",
    "ft_y=dataset['lable'].to_list\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting the weights from the training ###\n",
    "w2v_weights = torch.FloatTensor(model_w2v.wv.vectors)\n",
    "ft_weights = torch.FloatTensor(model_ft.wv.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_sz: int, hidden_sz: int, label_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        self.label_size = label_sz\n",
    "\n",
    "        self.U_i = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_i = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "\n",
    "        self.U_f = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_f = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "\n",
    "        self.U_c = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_c = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_c = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "\n",
    "        self.U_o = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_o = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "\n",
    "        self.hidden2label = nn.Parameter(torch.Tensor(hidden_sz, label_sz))\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "            stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "            for weight in self.parameters():\n",
    "                weight.data.uniform_(-stdv, stdv)\n",
    "                \n",
    "    \n",
    "    def forward(self, x, init_states=None):\n",
    "            \"\"\"\n",
    "            assumes x.shape represents (batch_size, sequence_size, input_size)\n",
    "            \"\"\"\n",
    "            seq_sz, embeded_sz = x.size()\n",
    "            \n",
    "            if init_states is None:\n",
    "                h_t, c_t = (\n",
    "                    torch.zeros(self.hidden_size).to(x.device),\n",
    "                    torch.zeros(self.hidden_size).to(x.device),\n",
    "                )\n",
    "            else:\n",
    "                h_t, c_t = init_states\n",
    "\n",
    "            #loop over all words in sequence\n",
    "            \n",
    "            for t in range(seq_sz):\n",
    "                x_t = x[t]\n",
    "                i_t = torch.sigmoid(x_t @ self.U_i + h_t @ self.V_i + self.b_i)\n",
    "                f_t = torch.sigmoid(x_t @ self.U_f + h_t @ self.V_f + self.b_f)\n",
    "                g_t = torch.tanh(x_t @ self.U_c + h_t @ self.V_c + self.b_c)\n",
    "                o_t = torch.sigmoid(x_t @ self.U_o + h_t @ self.V_o + self.b_o)\n",
    "                c_t = f_t * c_t + i_t * g_t\n",
    "                h_t = o_t * torch.tanh(c_t)\n",
    "                    \n",
    "                \n",
    "                # last word:\n",
    "                if t==seq_sz-1:\n",
    "                    dropout = nn.Dropout(p=0.2)\n",
    "                    out_ht = dropout(h_t)\n",
    "                    y_pred = torch.sigmoid(out_ht @ self.hidden2label)\n",
    "            \n",
    "            return  y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,input_sz: int, hidden_sz: int, label_sz: int, embedding_weights):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weights)\n",
    "        self.lstm = LSTM(input_sz, hidden_sz, label_sz)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x_embeded = self.embedding(x)\n",
    "        pred = self.lstm(x_embeded)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating the training loop\n",
    "'''\n",
    "def run_model_loop(X_input,y_input, data_size, test_p, classifier, epochs, criterion, optimizer,tb_dirname):\n",
    "    \n",
    "    X = X_input[:data_size]\n",
    "    y = y_input[:data_size]\n",
    "    y = torch.tensor(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_p, random_state=42)\n",
    "    \n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    train_auc_list=[]\n",
    "    test_auc_list=[]\n",
    "\n",
    "#     writer = SummaryWriter(log_dir=f'{tb_dir}/{tb_dirname}_{time.time()}')\n",
    "    j = 0\n",
    "\n",
    "    for e in range(epochs):\n",
    "        train_labels_list = []\n",
    "        train_prediction_list = []\n",
    "        test_labels_list = []\n",
    "        test_prediction_list = []\n",
    "\n",
    "        running_loss = 0\n",
    "        classifier.train()\n",
    "\n",
    "        for i, review in enumerate(X_train):\n",
    "            j+=1\n",
    "            label = y_train[i].view(1)\n",
    "            review, label = review.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            #use model to predict sentiment\n",
    "            pred = classifier(review).to(torch.float)\n",
    "            #to be safe \n",
    "            label = label.to(torch.float)\n",
    "            loss = criterion(pred, label)\n",
    "            #calculate loss\n",
    "            running_loss+=loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_prediction_list.append(pred.item())\n",
    "            train_labels_list.append(label.item())\n",
    "        \n",
    "#             writer.add_scalar(tag='loss/batch_loss_train', scalar_value=loss, global_step=j)\n",
    "\n",
    "        epoch_train_loss = running_loss/len(X_train)\n",
    "        train_loss_list.append(epoch_train_loss)\n",
    "        fpr_t, tpr_t, thresholds_t = roc_curve(np.array(train_labels_list), np.array(train_prediction_list))\n",
    "        train_auc = auc(fpr_t, tpr_t)\n",
    "        train_auc_list.append(train_auc)\n",
    "        \n",
    "        test_loss = 0\n",
    "        classifier.eval()\n",
    "        # we dont need to update weights, so we define no_grad() to save memory\n",
    "        with torch.no_grad():\n",
    "            for i, review in enumerate(X_test):\n",
    "                label= y_test[i].view(1)\n",
    "                review, label = review.to(device), label.to(device)\n",
    "                test_pred = classifier(review).to(torch.float)\n",
    "                label = label.to(torch.float)\n",
    "                loss=criterion(test_pred, label)\n",
    "                test_loss+=loss.item()\n",
    "                test_prediction_list.append(test_pred.item())\n",
    "                test_labels_list.append(label.item())\n",
    "        \n",
    "        epoch_test_loss = test_loss/len(X_test)\n",
    "        test_loss_list.append(epoch_test_loss)\n",
    "        fpr_v, tpr_v, thresholds_v = roc_curve(np.array(test_labels_list),np.array(test_prediction_list))\n",
    "        test_auc = auc(fpr_v, tpr_v)\n",
    "        test_auc_list.append(test_auc)\n",
    "        \n",
    "        #Tensorboard documentation\n",
    "        #loss\n",
    "#         writer.add_scalar(tag='loss/train', scalar_value=epoch_train_loss, global_step=e)\n",
    "#         writer.add_scalar(tag='loss/test', scalar_value=epoch_test_loss, global_step=e)\n",
    "#         #auc\n",
    "#         writer.add_scalar(tag='auc/train', scalar_value=train_auc, global_step=e)\n",
    "#         writer.add_scalar(tag='auc/test', scalar_value=test_auc, global_step=e)\n",
    "\n",
    "    return {'train_loss':train_loss_list,\n",
    "            'train_auc':train_auc_list,\n",
    "            'test_auc':test_auc_list,\n",
    "            'test_loss':test_loss_list,\n",
    "            'fpr_v':fpr_v,\n",
    "            'tpr_v':tpr_v\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([tensor([28776,   213,   944,  ...,     0,     0,     0]),\n",
       "       tensor([26430, 93382,   616,  ...,     0,     0,     0]),\n",
       "       tensor([  412,  3601, 17375,  ...,     0,     0,     0]), ...,\n",
       "       tensor([ 105, 4421,  132,  ...,    0,    0,    0]),\n",
       "       tensor([  23, 1093, 6660,  ...,    0,    0,    0]),\n",
       "       tensor([   3, 6818,   23,  ...,    0,    0,    0])], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_X.values\n",
    "# w2v_X = X_w2v_full.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "take(): argument 'index' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-e59ecbf72952>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m### Run loop for Word2Vec ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mrun_model_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw2v_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnet_w2v\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer_w2v\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"model_testing\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-363066ea1a7f>\u001b[0m in \u001b[0;36mrun_model_loop\u001b[1;34m(X_input, y_input, data_size, test_p, classifier, epochs, criterion, optimizer, tb_dirname)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtrain_loss_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2211\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[1;32m-> 2212\u001b[1;33m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[0;32m   2213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2211\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[1;32m-> 2212\u001b[1;33m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[0;32m   2213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[1;34m(X, indices)\u001b[0m\n\u001b[0;32m    214\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[0;32m    215\u001b[0m             \u001b[1;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: take(): argument 'index' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "criterion = nn.BCELoss()\n",
    "epochs = 10\n",
    "\n",
    "net_w2v = Net(50, 10, 1 , w2v_weights).to(device)\n",
    "net_ft = Net(50, 10, 1 , ft_weights).to(device)\n",
    "\n",
    "optimizer_w2v = optim.Adam(net_w2v.parameters(), lr = 0.01, weight_decay = 1e-3)\n",
    "optimizer_ft = optim.Adam(net_ft.parameters(), lr = 0.01, weight_decay = 1e-3)\n",
    "\n",
    "### Run loop for Word2Vec ###\n",
    "run_model_loop(w2v_X.values,w2v_y,2000,0.2,net_w2v,epochs,criterion,optimizer_w2v,\"model_testing\")\n",
    "\n",
    "\n",
    "### Run loop for FastText ###\n",
    "# run_model_loop(ft_X,ft_y,2000,0.2,net_ft,epochs,criterion,optimizer_ft,\"model_testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "w2v_weights = torch.FloatTensor(model_w2v.wv.vectors)\n",
    "input_dim = 28\n",
    "hidden_dim = 128\n",
    "layer_dim = 1  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\n",
    "output_dim = 2\n",
    " \n",
    "GRU_model = LSTM(input_dim, hidden_dim, layer_dim, output_dim,w2v_weights)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    " \n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "     \n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.BCEWithLogitsLoss ()\n",
    " \n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.01\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "def training_loop(X_input,y_input, data_size, test_p, classifier, epochs, criterion):\n",
    "    X=X_input[:data_size]\n",
    "    y = y_input[:data_size]\n",
    "    y=torch.FloatTensor(y)\n",
    "    #y=torch.LongTensor([label for label in label_input])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_p, random_state=42)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr = 0.01, weight_decay = 1e-3)\n",
    "    \n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    train_auc_list=[]\n",
    "    test_auc_list=[]\n",
    "\n",
    "    j = 0\n",
    "#     k_folds=5\n",
    "#     kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "#     for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "#         # Sample elements randomly from a given list of ids, no replacement.\n",
    "#         train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "#         test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "#         # Define data loaders for training and testing data in this fold\n",
    "#         trainloader = torch.utils.data.DataLoader(\n",
    "#                           dataset, \n",
    "#                           batch_size=10, sampler=train_subsampler)\n",
    "#         testloader = torch.utils.data.DataLoader(\n",
    "#                           dataset,\n",
    "#                           batch_size=10, sampler=test_subsampler) \n",
    "\n",
    "    for e in range(epochs):\n",
    "        train_labels_list = []\n",
    "        train_prediction_list = []\n",
    "        test_labels_list = []\n",
    "        test_prediction_list = []\n",
    "\n",
    "        running_loss = 0\n",
    "        classifier.train()\n",
    "\n",
    "        for i, review in enumerate(X_train):\n",
    "            \n",
    "            j+=1\n",
    "            label = y_train[i].view(1)\n",
    "            review, label = review.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            #use model to predict sentiment\n",
    "            pred = classifier(review).to(torch.float)\n",
    "            #to be safe \n",
    "            label = label.to(torch.float)\n",
    "            loss = criterion(pred, label)\n",
    "            #calculate loss\n",
    "            running_loss+=loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_prediction_list.append(pred.item())\n",
    "            train_labels_list.append(label.item())\n",
    "        \n",
    "#             writer.add_scalar(tag='loss/batch_loss_train', scalar_value=loss, global_step=j)\n",
    "\n",
    "        epoch_train_loss = running_loss/len(X_train)\n",
    "        train_loss_list.append(epoch_train_loss)\n",
    "        fpr_t, tpr_t, thresholds_t = roc_curve(np.array(train_labels_list), np.array(train_prediction_list))\n",
    "        train_auc = auc(fpr_t, tpr_t)\n",
    "        train_auc_list.append(train_auc)\n",
    "        \n",
    "        test_loss = 0\n",
    "        classifier.eval()\n",
    "        # we dont need to update weights, so we define no_grad() to save memory\n",
    "        with torch.no_grad():\n",
    "            for i, review in enumerate(X_test):\n",
    "                label= y_test[i].view(1)\n",
    "                review, label = review.to(device), label.to(device)\n",
    "                test_pred = classifier(review).to(torch.float)\n",
    "                label = label.to(torch.float)\n",
    "                loss=criterion(test_pred, label)\n",
    "                test_loss+=loss.item()\n",
    "                test_prediction_list.append(test_pred.item())\n",
    "                test_labels_list.append(label.item())\n",
    "        \n",
    "        epoch_test_loss = test_loss/len(X_test)\n",
    "        test_loss_list.append(epoch_test_loss)\n",
    "        fpr_v, tpr_v, thresholds_v = roc_curve(np.array(test_labels_list),np.array(test_prediction_list))\n",
    "        test_auc = auc(fpr_v, tpr_v)\n",
    "        test_auc_list.append(test_auc)\n",
    "        \n",
    "        #Tensorboard documentation\n",
    "        #loss\n",
    "#         writer.add_scalar(tag='loss/train', scalar_value=epoch_train_loss, global_step=e)\n",
    "#         writer.add_scalar(tag='loss/test', scalar_value=epoch_test_loss, global_step=e)\n",
    "#         #auc\n",
    "#         writer.add_scalar(tag='auc/train', scalar_value=train_auc, global_step=e)\n",
    "#         writer.add_scalar(tag='auc/test', scalar_value=test_auc, global_step=e)\n",
    "\n",
    "    return {'train_loss':train_loss_list,\n",
    "            'train_auc':train_auc_list,\n",
    "            'test_auc':test_auc_list,\n",
    "            'test_loss':test_loss_list,\n",
    "            'fpr_v':fpr_v,\n",
    "            'tpr_v':tpr_v\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_844/4088119371.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw2v_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGRU_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_844/1418490927.py\u001b[0m in \u001b[0;36mtraining_loop\u001b[1;34m(X_input, y_input, data_size, test_p, classifier, epochs, criterion)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;31m#use model to predict sentiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[1;31m#to be safe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_844/4038501710.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mhn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mhn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mouts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "train_model=training_loop(w2v_X,w2v_y, 1000, 0.2, GRU_model, 10, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
