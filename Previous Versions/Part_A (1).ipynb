{
 "cells":[
  {
   "cell_type":"markdown",
   "source":[
    "# Group 11 DL Project - Part A "
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Imports "
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "!pip install nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import gensim.downloader as api"
   ],
   "execution_count":32,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Requirement already satisfied: nltk in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (3.7)\r\n",
      "Requirement already satisfied: joblib in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from nltk) (1.1.0)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from nltk) (2022.3.15)\r\n",
      "Requirement already satisfied: click in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from nltk) (7.1.2)\r\n",
      "Requirement already satisfied: tqdm in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from nltk) (4.63.1)\r\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\r\n",
      "You should consider upgrading via the '\/opt\/python\/envs\/default\/bin\/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "**Preprocessing - tokenization**"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "def comments_tokenization(dir_path, bag_of_words):\n",
    "    #get all the files from the directory\n",
    "    dir_file_path = dir_path\n",
    "    onlyfiles = [f for f in listdir(dir_file_path) if isfile(join(dir_file_path, f))]\n",
    "    files_paths = [dir_file_path + \"\\\\\" + f for f in onlyfiles]\n",
    "    \n",
    "    comments_tokens = [] #[[tokens_from_1_file],[tokens_from_2_file]] e.t.c\n",
    "    regex = re.compile('[^a-zA-Z]') #regex for removing all the symbols that are not letters\n",
    "\n",
    "    #for each comment (=file)\n",
    "    for path_to_file in files_paths:\n",
    "        with open(path_to_file, encoding=\"utf8\") as f:\n",
    "            word_data = f.read() #the data from file with comments\n",
    "            words = word_data.split() \n",
    "            clean_list = [] #this list will contain only words that doesn't have symbols other than letters\n",
    "\n",
    "            #for each words in comment (=file)\n",
    "            for w in words:\n",
    "                clean_word_data = regex.sub('', w) #clean the word by regex\n",
    "                if clean_word_data != \" \" and len(clean_word_data) > 0:\n",
    "                    #cleaning the stopwords from the sentence\n",
    "                    clean_word_data = clean_word_data.lower()\n",
    "                    #checking that the word is not a stop word (=a word that shows up frequently)\n",
    "                    if clean_word_data not in en_stops:\n",
    "                        clean_list.append(clean_word_data) #creates a clean comment\n",
    "                        bag_of_words.append(clean_word_data)\n",
    "        comments_tokens.append(clean_list) \n",
    "\n",
    "    return comments_tokens, bag_of_words\n",
    "\n",
    "en_stops = set(stopwords.words('english'))\n",
    "bag_of_words = []\n",
    "bag_of_words_neg = []\n",
    "## Positive comments tokenization ##\n",
    "pos_dir_path = \"\\\\data\\\\notebook_files\\\\train\\\\pos\"\n",
    "pos_tokens_list, bag_of_words = comments_tokenization(pos_dir_path, bag_of_words)\n",
    "## Negative comments tokenization ##\n",
    "neg_dir_path = \"\\\\data\\\\notebook_files\\\\train\\\\neg\"\n",
    "neg_tokens_list, bag_of_words_neg = comments_tokenization(neg_dir_path, bag_of_words_neg)\n",
    "\n",
    "#Creating one bag of words in order to add new words to the embeddings\n",
    "for bag in bag_of_words_neg:\n",
    "    if bag not in bag_of_words:\n",
    "        bag_of_words.append(bag)\n"
   ],
   "execution_count":33,
   "outputs":[
    {
     "ename":"LookupError",
     "evalue":"LookupError: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https:\/\/www.nltk.org\/data.html\n\n  Attempted to load \u001b[93mcorpora\/stopwords\u001b[0m\n\n  Searched in:\n    - '\/home\/datalore\/nltk_data'\n    - '\/opt\/python\/envs\/default\/nltk_data'\n    - '\/opt\/python\/envs\/default\/share\/nltk_data'\n    - '\/opt\/python\/envs\/default\/lib\/nltk_data'\n    - '\/usr\/share\/nltk_data'\n    - '\/usr\/local\/share\/nltk_data'\n    - '\/usr\/lib\/nltk_data'\n    - '\/usr\/local\/lib\/nltk_data'\n**********************************************************************\n",
     "traceback":[
      "\u001b[0;31m---------------------------------------------------------------------------",
      "Traceback (most recent call last)",
      "    at line 31 in <module>",
      "    at line 121 in __getattr__(self, attr)",
      "    at line 86 in __load(self)",
      "    at line 81 in __load(self)",
      "    at line 583 in find(resource_name, paths)",
      "LookupError: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https:\/\/www.nltk.org\/data.html\n\n  Attempted to load \u001b[93mcorpora\/stopwords\u001b[0m\n\n  Searched in:\n    - '\/home\/datalore\/nltk_data'\n    - '\/opt\/python\/envs\/default\/nltk_data'\n    - '\/opt\/python\/envs\/default\/share\/nltk_data'\n    - '\/opt\/python\/envs\/default\/lib\/nltk_data'\n    - '\/usr\/share\/nltk_data'\n    - '\/usr\/local\/share\/nltk_data'\n    - '\/usr\/lib\/nltk_data'\n    - '\/usr\/local\/lib\/nltk_data'\n**********************************************************************\n"
     ],
     "output_type":"error"
    }
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Pre-Processing - Embeddings "
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "First, we will enable Torch Embedding Module & load pre-trained Embeddings "
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "info = api.info()\n",
    "\n",
    "\n",
    "# from gensim.models import FastText\n",
    "# sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "# model = FastText(vector_size=4, window=3, min_count=1)"
   ],
   "execution_count":17,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Loading Embedding models (Gilad)"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# word2vec_model = api.load('word2vec-google-news-300') # loading the word2vec embedding model from the gensim\n",
    "# glove_model = api.load('glove-wiki-gigaword-300') # loading the GLoVE embedding model from the gensim\n",
    "\n",
    "## אם תרצה להוריד ממש, אז שחרר את הגרסה למטה באפור\n",
    "# glove_path = api.load('glove-wiki-gigaword-300', return_path=True)  # If we want to save the model to a local file\n",
    "# print(glove_path)\n",
    "# try:\n",
    "#     glove_model = gensim.models.keyedvectors.load_word2vec_format(glove_path)\n",
    "# except:\n",
    "#     glove_model = gensim.models.keyedvectors.load_word2vec_format(glove_path, binary=True)\n",
    "\n",
    "embedding_models=[glove_model]\n",
    "\n",
    "embed_model=embedding_models[0]"
   ],
   "execution_count":18,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Load the model to torch Embedding"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "weights = torch.FloatTensor(embed_model.vectors)\n",
    "embedding = nn.Embedding.from_pretrained(weights)\n",
    "embedding.requires_grad_(False)\n",
    "print(embedding)\n",
    "\n",
    "for index, word in enumerate(embed_model.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}\/{len(embed_model.index_to_key)} is {word}\")"
   ],
   "execution_count":19,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Embedding(400000, 300)\n",
      "word #0\/400000 is the\n",
      "word #1\/400000 is ,\n",
      "word #2\/400000 is .\n",
      "word #3\/400000 is of\n",
      "word #4\/400000 is to\n",
      "word #5\/400000 is and\n",
      "word #6\/400000 is in\n",
      "word #7\/400000 is a\n",
      "word #8\/400000 is \"\n",
      "word #9\/400000 is 's\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "sen = \"good morning sir fdsfa\"\n",
    "representation = []\n",
    "for word in sen.split():\n",
    "    if word not in embed_model.key_to_index:\n",
    "        print(f\"{word} not an existing word in the model\")\n",
    "        continue\n",
    "    else:\n",
    "        word_id = torch.tensor(embed_model.key_to_index[word])  # ID of the word in the embedding\n",
    "    vec = embed_model[word]\n",
    "    representation.append(vec)\n",
    "    print(torch.equal(torch.Tensor(vec),embedding(word_id)))\n",
    "\n",
    "representation = np.asarray(representation)\n",
    "print(representation.shape)"
   ],
   "execution_count":20,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "True\n",
      "True\n",
      "True\n",
      "fdsfa not an existing word in the model\n",
      "(3, 300)\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "#Yuval - GRU\n",
    "\"\"\"\n",
    "class GRUNET(nn.Module):\n",
    "    def __init__(self, input_size,hidden_size):\n",
    "        super(GRUNET, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        self.tanh=nn.Tanh()\n",
    "        nn.init.kaiming_normal_(self.hidden.weight)\n",
    "\n",
    "        #layers\n",
    "        self.h = nn.Linear(self.hidden_size + self.input_size, self.hidden_size)\n",
    "        self.reset=nn.Linear(self.hidden_size + self.input_size, self.hidden_size)\n",
    "        self.update=nn.Linear(self.hidden_size + self.input_size, self.reset*self.hidden_size)\n",
    "        self.h_tilda=nn.Linear(self.hidden_size + self.input_size, self.reset*self.hidden_size)\n",
    "    \n",
    "\n",
    "    def forward(self, x, input_embeddings):\n",
    "    #initialize hidden output with zeroes\n",
    "        z=self.update\n",
    "        \n",
    "        \n",
    "        outputs,hidden_states=[],[]\n",
    "\n",
    "        output=torch.zeros(self.hidden_state,dtype=torch.long)\n",
    "        for val in input_embeddings:\n",
    "        \n",
    "            x=val\n",
    "            x=torch.cat((val,output))\n",
    "\n",
    "            x = self.hidden(x)\n",
    "            hidden_states.append(x)\n",
    "\n",
    "            x=nn.Sigmoid()\n",
    "        \n",
    "        outputs=torch.stack(outputs)\n",
    "        hidden_states=torch.stack(hidden_states)\n",
    "       \n",
    "        out = output[-1, :, :]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return outputs, hidden_states\n",
    "        \"\"\""
   ],
   "execution_count":5,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class GRUCell(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    An implementation of GRUCell.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.x2h = nn.Linear(input_size, 3 * hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, 3 * hidden_size, bias=bias)\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 \/ math.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        x = x.view(-1, x.size(1))\n",
    "        \n",
    "        gate_x = self.x2h(x) \n",
    "        gate_h = self.h2h(hidden)\n",
    "        \n",
    "        gate_x = gate_x.squeeze()\n",
    "        gate_h = gate_h.squeeze()\n",
    "        \n",
    "        i_r, i_i, i_n = gate_x.chunk(3, 1)\n",
    "        h_r, h_i, h_n = gate_h.chunk(3, 1)\n",
    "        \n",
    "        \n",
    "        resetgate = F.sigmoid(i_r + h_r)\n",
    "        inputgate = F.sigmoid(i_i + h_i)\n",
    "        newgate = F.tanh(i_n + (resetgate * h_n))\n",
    "        \n",
    "        hy = newgate + inputgate * (hidden - newgate)\n",
    "        \n",
    "        \n",
    "        return hy"
   ],
   "execution_count":21,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, bias=True):\n",
    "        super(GRUModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "         \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "         \n",
    "       \n",
    "        self.gru_cell = GRUCell(input_dim, hidden_dim, layer_dim)\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "     \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Initialize hidden state with zeros\n",
    "        #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "        #print(x.shape,\"x.shape\")100, 28, 28\n",
    "        if torch.cuda.is_available():\n",
    "            h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).cuda())\n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "         \n",
    "       \n",
    "        outs = []\n",
    "        \n",
    "        hn = h0[0,:,:]\n",
    "        \n",
    "        for seq in range(x.size(1)):\n",
    "            hn = self.gru_cell(x[:,seq,:], hn) \n",
    "            outs.append(hn)\n",
    "            \n",
    "\n",
    "        out = outs[-1].squeeze()\n",
    "        \n",
    "        out = self.fc(out) \n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    " "
   ],
   "execution_count":0,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28\n",
    "hidden_dim = 128\n",
    "layer_dim = 1  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\n",
    "output_dim = 10\n",
    " \n",
    "model = GRUModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    " \n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "     \n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    " \n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    " \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ],
   "execution_count":31,
   "outputs":[
    {
     "ename":"NameError",
     "evalue":"NameError: name 'GRUModel' is not defined",
     "traceback":[
      "\u001b[0;31m---------------------------------------------------------------------------",
      "Traceback (most recent call last)",
      "    at line 9 in <module>",
      "NameError: name 'GRUModel' is not defined"
     ],
     "output_type":"error"
    }
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    " \n",
    "# Number of steps to unroll\n",
    "seq_dim = 28 \n",
    "\n",
    "loss_list = []\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "          \n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            images = Variable(images.view(-1, seq_dim, input_dim))\n",
    "            labels = Variable(labels)\n",
    "          \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "         \n",
    "        # Forward pass to get output\/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            loss.cuda()\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_list.append(loss.item())\n",
    "        iter += 1\n",
    "         \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                if torch.cuda.is_available():\n",
    "                    images = Variable(images.view(-1, seq_dim, input_dim).cuda())\n",
    "                else:\n",
    "                    images = Variable(images.view(-1 , seq_dim, input_dim))\n",
    "                \n",
    "                # Forward pass only to get logits\/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                 \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                 \n",
    "                # Total correct predictions\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "             \n",
    "            accuracy = 100 * correct \/ total\n",
    "             \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ],
   "execution_count":0,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  }
 ],
 "metadata":{
  "datalore":{
   "version":1,
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[
    {
     "name":"nltk",
     "version":"3.7",
     "source":"PIP"
    }
   ]
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}