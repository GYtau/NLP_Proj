{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 11 DL Project - Part A "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "en_stops = set(stopwords.words('english'))\n",
    "####\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will make a new dataset from the files with positive and negative reviews\n",
    "\n",
    "def make_clean_comments(dir_path, comment_type):\n",
    "    #get all the files from the directory\n",
    "    onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n",
    "    files_paths = [dir_path + \"\\\\\" + f for f in onlyfiles]\n",
    "    \n",
    "    comments_tokens = [] #{[tokens_from_1_file]:0/1,[tokens_from_2_file]...} e.t.c\n",
    "    regex = re.compile('[^a-zA-Z]') #regex for removing all the symbols that are not letters\n",
    "    \n",
    "    #for each comment(=file in pos/neg directory)\n",
    "    for path_to_file in files_paths:\n",
    "        with open(path_to_file, encoding=\"utf8\") as f:\n",
    "            full_comment = f.read() #the data from file with comments\n",
    "            words = full_comment.split() \n",
    "            clean_list = [] #this list will contain only words that doesn't have symbols other than letters\n",
    "            \n",
    "            #for each words in comment (=file)\n",
    "            for w in words:\n",
    "                clean_word_data = regex.sub('', w) #clean the word by regex\n",
    "                \n",
    "                if len(clean_word_data) > 1:\n",
    "                    #cleaning the stopwords from the sentence\n",
    "                    clean_word_data = clean_word_data.lower()\n",
    "                    #checking that the word is not a stop word (=a word that shows up frequently)\n",
    "                    if clean_word_data not in en_stops:\n",
    "                        clean_list.append(clean_word_data) #creates a clean comment\n",
    "                        \n",
    "        comments_tokens.append(clean_list)\n",
    "        #creating list of comment type for the later creation of dataset\n",
    "    if comment_type == \"pos\":          \n",
    "        comments_type = [1 for i in range(len(comments_tokens))]\n",
    "    else:\n",
    "        comments_type = [0 for i in range(len(comments_tokens))]\n",
    "\n",
    "    return comments_tokens, comments_type\n",
    "\n",
    "## Positive comments tokenization ##\n",
    "pos_dir_path = fr'C:\\Users\\YuvalZiv\\Desktop\\nlp project\\train\\pos'\n",
    "pos_tokens_sentences, comments_type_list_pos = make_clean_comments(pos_dir_path, \"pos\")\n",
    "## Negative comments tokenization ##\n",
    "neg_dir_path = fr'C:\\Users\\YuvalZiv\\Desktop\\nlp project\\train\\neg'\n",
    "neg_tokens_sentences, comments_type_list_neg = make_clean_comments(neg_dir_path, \"neg\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bromwell, high, cartoon, comedy, ran, time, p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[homelessness, houselessness, george, carlin, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[brilliant, overacting, lesley, ann, warren, b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[easily, underrated, film, inn, brooks, cannon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[typical, mel, brooks, film, much, less, slaps...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18745</th>\n",
       "      <td>[towards, end, movie, felt, technical, felt, l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18746</th>\n",
       "      <td>[kind, movie, enemies, content, watch, time, b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18747</th>\n",
       "      <td>[saw, descent, last, night, stockholm, film, f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18748</th>\n",
       "      <td>[films, pick, pound, turn, rather, good, rd, c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18749</th>\n",
       "      <td>[one, dumbest, films, ive, ever, seen, rips, n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37500 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  lable\n",
       "0      [bromwell, high, cartoon, comedy, ran, time, p...      1\n",
       "1      [homelessness, houselessness, george, carlin, ...      1\n",
       "2      [brilliant, overacting, lesley, ann, warren, b...      1\n",
       "3      [easily, underrated, film, inn, brooks, cannon...      1\n",
       "4      [typical, mel, brooks, film, much, less, slaps...      1\n",
       "...                                                  ...    ...\n",
       "18745  [towards, end, movie, felt, technical, felt, l...      0\n",
       "18746  [kind, movie, enemies, content, watch, time, b...      0\n",
       "18747  [saw, descent, last, night, stockholm, film, f...      0\n",
       "18748  [films, pick, pound, turn, rather, good, rd, c...      0\n",
       "18749  [one, dumbest, films, ive, ever, seen, rips, n...      0\n",
       "\n",
       "[37500 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preparing the labled data\n",
    "data_pos = {'comment':pos_tokens_sentences,\n",
    "        'lable':comments_type_list_pos}\n",
    "data_neg = {'comment':neg_tokens_sentences,\n",
    "        'lable':comments_type_list_neg}\n",
    "\n",
    "#creating a dataset of all the data together\n",
    "dataset = pd.DataFrame(data_pos)\n",
    "df_temp = pd.DataFrame(data_neg)\n",
    "dataset.append(df_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a train set -> I tried also with random and stuff and it didn't work :(\n",
    "k_folds=5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=10, sampler=train_subsampler)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=10, sampler=test_subsampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.sampler.SubsetRandomSampler at 0x16664000f70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subsampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
